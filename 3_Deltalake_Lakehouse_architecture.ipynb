{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOX8TtZ/kFwnO++eVWyfxJw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samas-it-services/open-course-delta-lake/blob/feature%2Fblockchain-via-delta-lake/3_Deltalake_Lakehouse_architecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwpgDQifQOHA",
        "outputId": "f18d50d8-d321-42fd-d8f7-a9d166b06647"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting delta-spark\n",
            "  Downloading delta_spark-3.2.0-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: pyspark<3.6.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from delta-spark) (3.5.1)\n",
            "Requirement already satisfied: importlib-metadata>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from delta-spark) (7.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=1.0.0->delta-spark) (3.19.2)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark<3.6.0,>=3.5.0->delta-spark) (0.10.9.7)\n",
            "Installing collected packages: delta-spark\n",
            "Successfully installed delta-spark-3.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -q pyspark\n",
        "# !pip install delta-spark==4.0.0rc1\n",
        "!pip install delta-spark\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmjchqKXc4Wh",
        "outputId": "a4ad37e5-1080-43dd-a190-3394cf81d2f7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selecting previously unselected package libxtst6:amd64.\n",
            "(Reading database ... 121925 files and directories currently installed.)\n",
            "Preparing to unpack .../libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u412-ga-1~22.04.1_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u412-ga-1~22.04.1) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jdk-headless_8u412-ga-1~22.04.1_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u412-ga-1~22.04.1) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u412-ga-1~22.04.1) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u412-ga-1~22.04.1) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def _create_delta_spark():\n",
        "  from pyspark.sql import SparkSession\n",
        "  from delta import configure_spark_with_delta_pip\n",
        "  builder = SparkSession.builder.appName(\"DeltaLakeApp\") \\\n",
        "  .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
        "  .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\\\n",
        "  .config(\"spark.jars.packages\",\"io.delta:delta-core_2.12:2.0.0\")\n",
        "  return configure_spark_with_delta_pip(builder).getOrCreate()\n",
        "\n",
        "spark = _create_delta_spark()\n",
        "# Enable Delta Lake features\n",
        "spark.conf.set(\"spark.databricks.delta.preview.enabled\", \"true\")\n"
      ],
      "metadata": {
        "id": "eNrqGeEJZXaQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _enable_sparkui(port=4040):\n",
        "    from google.colab import output\n",
        "    return output.serve_kernel_port_as_window(port, path='/jobs/index.html')\n",
        "\n",
        "_enable_sparkui()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "N95mRRGdZNnz",
        "outputId": "300bcca0-a735-4028-b921-2f4a59d1d244"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, text, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port);\n",
              "    const anchor = document.createElement('a');\n",
              "    anchor.href = new URL(path, url).toString();\n",
              "    anchor.target = '_blank';\n",
              "    anchor.setAttribute('data-href', url + path);\n",
              "    anchor.textContent = text;\n",
              "    element.appendChild(anchor);\n",
              "  })(4040, \"/jobs/index.html\", \"https://localhost:4040/jobs/index.html\", window.element)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!java -version\n",
        "!which java\n",
        "\n",
        "# Once inside the file, set the variable with your Java path, then save and close the file\n",
        "%env JAVA_HOME=/usr/bin/java\n",
        "\n",
        "# Test if it was set successfully\n",
        "!echo \"JAVA_HOME=${JAVA_HOME}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlLHfEPmdO5p",
        "outputId": "d071b759-9f7e-458e-a46f-29268c8f783a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"11.0.23\" 2024-04-16\n",
            "OpenJDK Runtime Environment (build 11.0.23+9-post-Ubuntu-1ubuntu122.04.1)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.23+9-post-Ubuntu-1ubuntu122.04.1, mixed mode, sharing)\n",
            "/usr/bin/java\n",
            "env: JAVA_HOME=/usr/bin/java\n",
            "JAVA_HOME=/usr/bin/java\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "swkpxhvoUgeF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "from delta import *\n",
        "from delta.tables import *\n",
        "import os\n",
        "\n",
        "class StorageInterface:\n",
        "    def save_df(self, df, path):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def read_df(self, path):\n",
        "        raise NotImplementedError\n",
        "\n",
        "class FileSystemStorage(StorageInterface):\n",
        "    def save_df(self, df, path):\n",
        "        df.write.format(\"delta\").mode('overwrite').save(path)\n",
        "\n",
        "    def read_df(self, path):\n",
        "        return spark.read.format(\"delta\").load(path)\n",
        "\n",
        "class S3Storage(StorageInterface):\n",
        "    def __init__(self, bucket_name):\n",
        "        self.bucket_name = bucket_name\n",
        "\n",
        "    def save_df(self, df, path):\n",
        "        s3_path = f\"s3a://{self.bucket_name}/{path}\"\n",
        "        df.write.format(\"delta\").mode('overwrite').save(s3_path)\n",
        "\n",
        "    def read_df(self, path):\n",
        "        s3_path = f\"s3a://{self.bucket_name}/{path}\"\n",
        "        return spark.read.format(\"delta\").load(s3_path)\n",
        "\n",
        "class DatabaseStorage(StorageInterface):\n",
        "    def __init__(self, jdbc_url, table, properties):\n",
        "        self.jdbc_url = jdbc_url\n",
        "        self.table = table\n",
        "        self.properties = properties\n",
        "\n",
        "    def save_df(self, df, path):\n",
        "        df.write.jdbc(self.jdbc_url, self.table, mode='overwrite', properties=self.properties)\n",
        "\n",
        "    def read_df(self, path):\n",
        "        return spark.read.jdbc(self.jdbc_url, self.table, properties=self.properties)\n",
        "\n",
        "\n",
        "print(\"Define storage layers\")\n",
        "bronze_layer = FileSystemStorage()\n",
        "silver_layer = FileSystemStorage()\n",
        "gold_layer = FileSystemStorage()\n",
        "\n",
        "print(\"Sample data for bronze layer\")\n",
        "data = [(\"Mr. Bilgrami\", 48), (\"Mr. AZ\", 51), (\"Mr. Shyam\", 44), (\"Mr. Taimur\", 25)]\n",
        "columns = [\"Name\", \"Age\"]\n",
        "df_bronze = spark.createDataFrame(data, columns)\n",
        "\n",
        "print(\"Save bronze data\")\n",
        "bronze_layer.save_df(df_bronze, \"bronze/people\")\n",
        "\n",
        "print(\"Read bronze data\")\n",
        "df_bronze = bronze_layer.read_df(\"bronze/people\")\n",
        "\n",
        "print(\"Showing the bronze data\")\n",
        "df_bronze.show()\n",
        "\n",
        "print(\"Transform bronze to silver (e.g., filter data)\")\n",
        "df_silver = df_bronze.filter(col(\"Age\") > 30)\n",
        "\n",
        "print(\"Save silver data as Delta format\")\n",
        "silver_layer.save_df(df_silver, \"silver/people\")\n",
        "\n",
        "print(\"Read silver data\")\n",
        "df_silver = silver_layer.read_df(\"silver/people\")\n",
        "\n",
        "print(\"Showing the silver data where Age > 30\")\n",
        "df_silver.show()\n",
        "\n",
        "print(\"Transform silver to gold (e.g., select specific columns)\")\n",
        "df_gold = df_silver.select(\"Name\")\n",
        "\n",
        "print(\"Save gold data as Delta format with advanced features\")\n",
        "gold_table_path = \"gold/people\"\n",
        "df_gold.write.format(\"delta\").mode('overwrite').save(gold_table_path)\n",
        "\n",
        "print(\"Create DeltaTable object\")\n",
        "gold_table = DeltaTable.forPath(spark, gold_table_path)\n",
        "\n",
        "print(\"Coordinated commits\")\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "\n",
        "new_data = [(\"David\", 35)]\n",
        "schema = StructType([StructField(\"Name\", StringType(), True), StructField(\"Age\", IntegerType(), True)])\n",
        "df_new_data = spark.createDataFrame(new_data, schema)\n",
        "\n",
        "# Using merge to demonstrate coordinated commits\n",
        "gold_table.alias(\"gold\").merge(\n",
        "    df_new_data.alias(\"updates\"),\n",
        "    \"gold.Name = updates.Name\"\n",
        ").whenMatchedUpdate(set={\"Age\": \"updates.Age\"}) \\\n",
        " .whenNotMatchedInsert(values={\"Name\": \"updates.Name\", \"Age\": \"updates.Age\"}) \\\n",
        " .execute()\n",
        "\n",
        "print(\"Schema evolution using type widening\")\n",
        "new_data_variant = [(\"Emma\", \"thirty\")]\n",
        "schema_variant = StructType([StructField(\"Name\", StringType(), True), StructField(\"Age\", StringType(), True)])\n",
        "df_new_data_variant = spark.createDataFrame(new_data_variant, schema_variant)\n",
        "\n",
        "df_new_data_variant.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").save(gold_table_path)\n",
        "\n",
        "print(\"Liquid clustering\")\n",
        "gold_table.optimize().executeCompaction()\n",
        "\n",
        "print(\"Showing the gold data\")\n",
        "df_gold = gold_layer.read_df(gold_table_path)\n",
        "df_gold.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8vPcEh6UhZZ",
        "outputId": "9b0447c3-b7c8-4150-cbe3-d5915e7b4102"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Define storage layers\n",
            "Sample data for bronze layer\n",
            "Save bronze data\n",
            "Read bronze data\n",
            "Showing the bronze data\n",
            "+------------+---+\n",
            "|        Name|Age|\n",
            "+------------+---+\n",
            "|   Mr. Shyam| 44|\n",
            "|  Mr. Taimur| 25|\n",
            "|Mr. Bilgrami| 48|\n",
            "|      Mr. AZ| 51|\n",
            "+------------+---+\n",
            "\n",
            "Transform bronze to silver (e.g., filter data)\n",
            "Save silver data as Delta format\n",
            "Read silver data\n",
            "Showing the silver data where Age > 30\n",
            "+------------+---+\n",
            "|        Name|Age|\n",
            "+------------+---+\n",
            "|   Mr. Shyam| 44|\n",
            "|Mr. Bilgrami| 48|\n",
            "|      Mr. AZ| 51|\n",
            "+------------+---+\n",
            "\n",
            "Transform silver to gold (e.g., select specific columns)\n",
            "Save gold data as Delta format with advanced features\n",
            "Create DeltaTable object\n",
            "Coordinated commits\n",
            "Schema evolution using type widening\n",
            "Liquid clustering\n",
            "Showing the gold data\n",
            "+------------+------+\n",
            "|        Name|   Age|\n",
            "+------------+------+\n",
            "|        Emma|thirty|\n",
            "|       David|    35|\n",
            "|   Mr. Shyam|  NULL|\n",
            "|Mr. Bilgrami|  NULL|\n",
            "|      Mr. AZ|  NULL|\n",
            "+------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W5_HOFmzXOFO"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}