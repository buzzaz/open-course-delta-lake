{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Delta Lake Quick Start\n",
        "This notebook provides a quick start guide to using Delta Lake, as outlined in the [Delta Lake Documentation](https://docs.delta.io/latest/quick-start.html).\n"
      ],
      "metadata": {
        "id": "rwPBbktNW4JW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing Dependencies\n",
        "\n",
        "This code installs the necessary dependencies for working with Apache Spark and Delta Lake. Specifically, it installs:\n",
        "\n",
        "- `pyspark==3.5.1`: The Python interface for Apache Spark, which is a unified analytics engine for large-scale data processing.\n",
        "- `delta-spark`: The Delta Lake library, which is an open-source storage layer that brings reliability to data lakes.\n",
        "\n",
        "These dependencies are installed using the `pip` package manager. The `!` prefix is used to execute shell commands within the notebook environment."
      ],
      "metadata": {
        "id": "sBAZ5kNycIYY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVTcjqttVjlt",
        "outputId": "fa07f3f3-e959-4185-a57c-5604d3d74380"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark==3.5.1 in /usr/local/lib/python3.10/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark==3.5.1) (0.10.9.7)\n",
            "Requirement already satisfied: delta-spark in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
            "Requirement already satisfied: pyspark<3.6.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from delta-spark) (3.5.1)\n",
            "Requirement already satisfied: importlib-metadata>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from delta-spark) (8.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=1.0.0->delta-spark) (3.19.2)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark<3.6.0,>=3.5.0->delta-spark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "# Install necessary dependencies\n",
        "!pip install pyspark==3.5.1 #4.0.0.dev1\n",
        "!pip install delta-spark\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Libraries\n",
        "\n",
        "This code imports the required libraries for working with Apache Spark and Delta Lake.\n",
        "\n",
        "### Imports\n",
        "\n",
        "- `from pyspark.sql import SparkSession`\n",
        "  - This line imports the `SparkSession` class from the `pyspark.sql` module. `SparkSession` is the entry point for creating a Spark application and interacting with Spark's functionalities.\n",
        "\n",
        "- `from delta import *`\n",
        "  - This line imports all functions and classes from the `delta` module, which is part of the Delta Lake library. This library provides utilities for working with Delta Lake, a storage layer that brings ACID transactions, scalability, and performance to data lakes.\n",
        "\n",
        "By importing these libraries, you gain access to the necessary classes and functions for creating Spark sessions, reading and writing data, and interacting with Delta Lake."
      ],
      "metadata": {
        "id": "tPYfOeYdfHZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from delta import *\n"
      ],
      "metadata": {
        "id": "5dWjuR5NXBQR"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a Spark Session with Delta Lake\n",
        "\n",
        "This code creates a Spark session and configures it to work with Delta Lake.\n",
        "\n",
        "### Steps\n",
        "\n",
        "1. **Create a Spark Session Builder**\n",
        "   - `builder = SparkSession.builder.appName(\"DeltaLakeQuickStart\")`\n",
        "     - This line creates a `SparkSession` builder with the application name set to `\"DeltaLakeQuickStart\"`.\n",
        "\n",
        "2. **Configure Spark Session for Delta Lake**\n",
        "   - `builder.config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")`\n",
        "     - This line adds a configuration to the Spark session builder, enabling the Delta Lake SQL extension.\n",
        "   - `builder.config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")`\n",
        "     - This line configures the Spark session to use the Delta Lake catalog for managing tables and databases.\n",
        "\n",
        "3. **Create and Configure Spark Session with Delta Lake**\n",
        "   - `spark = configure_spark_with_delta_pip(builder).getOrCreate()`\n",
        "     - This line creates and configures the Spark session using the `configure_spark_with_delta_pip` function from the Delta Lake library.\n",
        "     - The `getOrCreate()` method creates a new Spark session or returns an existing one if it has already been created.\n",
        "\n",
        "By configuring the Spark session with the Delta Lake extensions and catalog, you enable Delta Lake capabilities within your Spark application. This includes features like ACID transactions, data versioning, and improved performance for data lake workloads."
      ],
      "metadata": {
        "id": "_ynjpDOsfCeK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Spark Session with Delta Lake\n",
        "builder = SparkSession.builder.appName(\"DeltaLakeQuickStart\") \\\n",
        "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
        "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n"
      ],
      "metadata": {
        "id": "WrEqla6GXDFT"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a Delta Table\n",
        "\n",
        "This code creates a Delta table using Apache Spark and the Delta Lake library.\n",
        "\n",
        "### Steps\n",
        "\n",
        "1. **Generate Sample Data**\n",
        "   - `data = spark.range(0, 5)`\n",
        "     - This line creates a DataFrame `data` containing a range of integers from 0 to 4 (inclusive) using the `spark.range` function.\n",
        "\n",
        "2. **Write Data to Delta Table**\n",
        "   - `data.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/delta-table\")`\n",
        "     - This line writes the `data` DataFrame to a Delta table located at the `/tmp/delta-table` path.\n",
        "     - The `write` method is used to initiate the writing process.\n",
        "     - `format(\"delta\")` specifies that the data should be written in the Delta Lake format.\n",
        "     - `mode(\"overwrite\")` sets the write mode to `overwrite`, which means that any existing data in the Delta table will be replaced by the new data.\n",
        "     - `save(\"/tmp/delta-table\")` saves the data to the specified path, creating a new Delta table or overwriting an existing one.\n",
        "\n",
        "The resulting Delta table will be stored in the `/tmp/delta-table` directory, and it will contain the integer values from 0 to 4. Delta Lake tables provide additional features like ACID transactions, data versioning, and efficient data storage and retrieval compared to traditional file-based data lakes."
      ],
      "metadata": {
        "id": "T82hk5mPhQ9b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Delta Table\n",
        "data = spark.range(0, 5)\n",
        "data.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/delta-table\")\n"
      ],
      "metadata": {
        "id": "Ro0IwOg3XM_T"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reading from the Delta Table\n",
        "\n",
        "This code reads data from the previously created Delta table and displays the contents.\n",
        "\n",
        "### Steps\n",
        "\n",
        "1. **Read Data from Delta Table**\n",
        "   - `df = spark.read.format(\"delta\").load(\"/tmp/delta-table\")`\n",
        "     - This line reads data from the Delta table located at `/tmp/delta-table`.\n",
        "     - The `read` method is used to initiate the reading process.\n",
        "     - `format(\"delta\")` specifies that the data should be read from a Delta Lake table.\n",
        "     - `load(\"/tmp/delta-table\")` loads the data from the specified path, which contains the Delta table.\n",
        "     - The resulting data is stored in the `df` DataFrame.\n",
        "\n",
        "2. **Display DataFrame Contents**\n",
        "   - `df.show()`\n",
        "     - This line prints the contents of the `df` DataFrame to the console or notebook output.\n",
        "\n",
        "By reading data from the Delta table, you can leverage the features provided by Delta Lake, such as data versioning and efficient data retrieval. The `show()` method allows you to inspect the contents of the DataFrame, ensuring that the data was read correctly from the Delta table."
      ],
      "metadata": {
        "id": "waGEaey8hp6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read from the Delta Table\n",
        "df = spark.read.format(\"delta\").load(\"/tmp/delta-table\")\n",
        "print(\"value of latest version of the delta table\")\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2VpEZw3XOjT",
        "outputId": "ee7891f0-ed98-4774-9bd8-1c898ab99319"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "value of latest version of the delta table\n",
            "+---+\n",
            "| id|\n",
            "+---+\n",
            "|  2|\n",
            "|  3|\n",
            "|  4|\n",
            "|  0|\n",
            "|  1|\n",
            "+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Updating the Delta Table\n",
        "\n",
        "This code updates the existing Delta table by overwriting it with new data.\n",
        "\n",
        "### Steps\n",
        "\n",
        "1. **Generate New Sample Data**\n",
        "   - `data = spark.range(5, 10)`\n",
        "     - This line creates a new DataFrame `data` containing a range of integers from 5 to 9 (inclusive) using the `spark.range` function.\n",
        "\n",
        "2. **Overwrite Delta Table with New Data**\n",
        "   - `data.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/delta-table\")`\n",
        "     - This line overwrites the existing Delta table located at `/tmp/delta-table` with the new `data` DataFrame.\n",
        "     - The `write` method is used to initiate the writing process.\n",
        "     - `format(\"delta\")` specifies that the data should be written in the Delta Lake format.\n",
        "     - `mode(\"overwrite\")` sets the write mode to `overwrite`, which means that any existing data in the Delta table will be replaced by the new data.\n",
        "     - `save(\"/tmp/delta-table\")` saves the new data to the specified path, overwriting the existing Delta table.\n",
        "\n",
        "After executing this code, the Delta table at `/tmp/delta-table` will no longer contain the initial range of integers from 0 to 4. Instead, it will be overwritten with the new range of integers from 5 to 9.\n",
        "\n",
        "Delta Lake's support for ACID transactions ensures that the overwrite operation is atomic, consistent, isolated, and durable, maintaining data integrity during the update process."
      ],
      "metadata": {
        "id": "s9GHl3fdh9eN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Update the Delta Table\n",
        "data = spark.range(5, 10)\n",
        "data.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/delta-table\")\n",
        "print(\"updated value of delta table\")\n",
        "df.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "sKAQ9UIBXTEK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bc369f1-c816-4c98-ba95-f4be15a05ebd"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updated value of delta table\n",
            "+---+\n",
            "| id|\n",
            "+---+\n",
            "|  7|\n",
            "|  8|\n",
            "|  9|\n",
            "|  5|\n",
            "|  6|\n",
            "+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reading the Updated Delta Table\n",
        "\n",
        "This code reads the updated Delta table and displays its contents.\n",
        "\n",
        "### Steps\n",
        "\n",
        "1. **Read Data from the Updated Delta Table**\n",
        "   - `df = spark.read.format(\"delta\").load(\"/tmp/delta-table\")`\n",
        "     - This line reads data from the updated Delta table located at `/tmp/delta-table`.\n",
        "     - The `read` method is used to initiate the reading process.\n",
        "     - `format(\"delta\")` specifies that the data should be read from a Delta Lake table.\n",
        "     - `load(\"/tmp/delta-table\")` loads the data from the specified path, which now contains the updated Delta table.\n",
        "     - The resulting data is stored in the `df` DataFrame.\n",
        "\n",
        "2. **Display DataFrame Contents**\n",
        "   - `df.show()`\n",
        "     - This line prints the contents of the `df` DataFrame to the console or notebook output.\n",
        "\n",
        "By reading the updated Delta table, you can verify that the overwrite operation was successful, and the table now contains the new range of integers from 5 to 9.\n",
        "\n",
        "The `show()` method displays the contents of the DataFrame, allowing you to inspect the data and ensure that the update was performed correctly. Delta Lake's transaction log and data versioning capabilities help maintain data integrity and enable time travel queries, if needed."
      ],
      "metadata": {
        "id": "fS2B_9Srh5Ze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the Updated Delta Table\n",
        "df = spark.read.format(\"delta\").load(\"/tmp/delta-table\")\n",
        "print(\"read values of most recent version of the delta table\")\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAq9vT_zXX2Z",
        "outputId": "c8327aae-e6be-41b0-97c3-fc7f0c28569f"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "read values of most recent version of the delta table\n",
            "+---+\n",
            "| id|\n",
            "+---+\n",
            "|  7|\n",
            "|  8|\n",
            "|  9|\n",
            "|  5|\n",
            "|  6|\n",
            "+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conditional update without overwrite\n",
        "Delta Lake provides programmatic APIs to conditional update, delete, and merge (upsert) data into tables. Here are a few examples."
      ],
      "metadata": {
        "id": "Ptxun4E4X9gI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Delta Table Operations\n",
        "\n",
        "This code demonstrates various operations that can be performed on a Delta table, such as updating, deleting, and upserting (merging) data. It also showcases the use of the Delta Table API provided by the Delta Lake library.\n",
        "\n",
        "### Imports\n",
        "\n",
        "- `from delta.tables import *`\n",
        "  - This line imports the necessary classes and functions from the `delta.tables` module, which provides the Delta Table API.\n",
        "- `from pyspark.sql.functions import *`\n",
        "  - This line imports various functions from the `pyspark.sql.functions` module, which are used for performing data transformations and manipulations.\n",
        "\n",
        "### Steps\n",
        "\n",
        "1. **Create a DeltaTable Object**\n",
        "   - `deltaTable = DeltaTable.forPath(spark, \"/tmp/delta-table\")`\n",
        "     - This line creates a `DeltaTable` object `deltaTable` by pointing to the existing Delta table located at `/tmp/delta-table`.\n",
        "\n",
        "2. **Update Even Values**\n",
        "   - `deltaTable.update(condition = expr(\"id % 2 == 0\"), set = { \"id\": expr(\"id + 100\") })`\n",
        "     - This line updates the Delta table by adding 100 to the `id` column for all rows where `id` is even.\n",
        "     - The `update` method is used to perform the update operation.\n",
        "     - `condition = expr(\"id % 2 == 0\")` specifies the condition for which rows should be updated, in this case, rows where `id` is even.\n",
        "     - `set = { \"id\": expr(\"id + 100\") }` defines the update expression, which adds 100 to the `id` column.\n",
        "\n",
        "3. **Delete Even Values**\n",
        "   - `deltaTable.delete(condition = expr(\"id % 2 == 0\"))`\n",
        "     - This line deletes all rows from the Delta table where `id` is even.\n",
        "     - The `delete` method is used to perform the deletion operation.\n",
        "     - `condition = expr(\"id % 2 == 0\")` specifies the condition for which rows should be deleted, in this case, rows where `id` is even.\n",
        "\n",
        "4. **Upsert (Merge) New Data**\n",
        "   - `newData = spark.range(0, 20)`\n",
        "     - This line creates a new DataFrame `newData` containing a range of integers from 0 to 19.\n",
        "   - `deltaTable.alias(\"oldData\") ...`\n",
        "     - This block of code performs an upsert (merge) operation on the Delta table, merging the `newData` DataFrame with the existing data.\n",
        "     - The `alias` method is used to assign aliases to the Delta table (`\"oldData\"`) and the new data (`\"newData\"`).\n",
        "     - The `merge` method initiates the merge operation, comparing the `\"id\"` column from both datasets. We use `upsert` method which combines an `insert` with an `update` operation, to improve data integrity and simplify any potential rollback. the For additional documentation on the `upsert` method, kindly refer to the Delta Lake documentation at https://docs.delta.io/latest/delta-update.html#upsert-into-a-table-using-merge\n",
        "     - `whenMatchedUpdate` specifies the update logic when a row in `\"oldData\"` matches a row in `\"newData\"`.\n",
        "     - `whenNotMatchedInsert` specifies the insert logic when a row in `\"newData\"` does not match any row in `\"oldData\"`.\n",
        "     - The `execute` method finalizes and applies the merge operation to the Delta table.\n",
        "\n",
        "5. **Display Updated Delta Table**\n",
        "   - `deltaTable.toDF().show()`\n",
        "     - This line converts the `deltaTable` object to a Spark DataFrame using `toDF()` and displays its contents using the `show()` method.\n",
        "\n",
        "By executing this code, you will see the Delta table being updated, deleted, and merged with new data. The final state of the Delta table will be displayed, reflecting the changes made through these operations."
      ],
      "metadata": {
        "id": "ZgNLKDOzitve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from delta.tables import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "deltaTable = DeltaTable.forPath(spark, \"/tmp/delta-table\")\n",
        "\n",
        "# Update every even value by adding 100 to it\n",
        "deltaTable.update(\n",
        "  condition = expr(\"id % 2 == 0\"),\n",
        "  set = { \"id\": expr(\"id + 100\") })\n",
        "\n",
        "# Delete every even value\n",
        "deltaTable.delete(condition = expr(\"id % 2 == 0\"))\n",
        "\n",
        "# Upsert (merge) new data\n",
        "newData = spark.range(0, 20)\n",
        "print(\"show dataframe newData\")\n",
        "newData.show()\n",
        "\n",
        "print(\"show table after update\")\n",
        "deltaTable.toDF().show()\n",
        "\n",
        "deltaTable.alias(\"oldData\") \\\n",
        "  .merge(\n",
        "    newData.alias(\"newData\"),\n",
        "    \"oldData.id = newData.id\") \\\n",
        "  .whenMatchedUpdate(set = { \"id\": col(\"newData.id\") }) \\\n",
        "  .whenNotMatchedInsert(values = { \"id\": col(\"newData.id\") }) \\\n",
        "  .execute()\n",
        "\n",
        "print(\"show upserted, i.e., merged and inserted table as a DataFrame\")\n",
        "deltaTable.toDF().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpRomij7X7UN",
        "outputId": "53024f3d-e085-4c58-f60b-0db0ca5f0098"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "show dataframe newData\n",
            "+---+\n",
            "| id|\n",
            "+---+\n",
            "|  0|\n",
            "|  1|\n",
            "|  2|\n",
            "|  3|\n",
            "|  4|\n",
            "|  5|\n",
            "|  6|\n",
            "|  7|\n",
            "|  8|\n",
            "|  9|\n",
            "| 10|\n",
            "| 11|\n",
            "| 12|\n",
            "| 13|\n",
            "| 14|\n",
            "| 15|\n",
            "| 16|\n",
            "| 17|\n",
            "| 18|\n",
            "| 19|\n",
            "+---+\n",
            "\n",
            "show table after update\n",
            "+---+\n",
            "| id|\n",
            "+---+\n",
            "|  7|\n",
            "|  9|\n",
            "|  5|\n",
            "+---+\n",
            "\n",
            "show upserted, i.e., merged and inserted table as a DataFrame\n",
            "+---+\n",
            "| id|\n",
            "+---+\n",
            "|  0|\n",
            "|  1|\n",
            "|  2|\n",
            "|  3|\n",
            "|  4|\n",
            "|  5|\n",
            "|  6|\n",
            "|  7|\n",
            "|  8|\n",
            "|  9|\n",
            "| 10|\n",
            "| 11|\n",
            "| 12|\n",
            "| 13|\n",
            "| 14|\n",
            "| 15|\n",
            "| 16|\n",
            "| 17|\n",
            "| 18|\n",
            "| 19|\n",
            "+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should see that some of the existing rows have been updated and new rows have been inserted.\n",
        "\n",
        "For more information on these operations, see Table deletes, updates, and merges.\n",
        "\n"
      ],
      "metadata": {
        "id": "s-rCbLVNYGtH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Querying a Specific Version of the Delta Table\n",
        "\n",
        "This code demonstrates how to query a specific version of a Delta table using the Delta Lake library's time travel capabilities.\n",
        "\n",
        "### Steps\n",
        "\n",
        "1. **Read Specific Version of Delta Table**\n",
        "   - `df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(\"/tmp/delta-table\")`\n",
        "     - This line reads data from the Delta table located at `/tmp/delta-table`.\n",
        "     - The `read` method is used to initiate the reading process.\n",
        "     - `format(\"delta\")` specifies that the data should be read from a Delta Lake table.\n",
        "     - `option(\"versionAsOf\", 0)` sets the `versionAsOf` option to `0`, which instructs Delta Lake to read the data from the initial version (version 0) of the table.\n",
        "     - `load(\"/tmp/delta-table\")` loads the data from the specified path, which contains the Delta table.\n",
        "     - The resulting data is stored in the `df` DataFrame.\n",
        "\n",
        "2. **Display DataFrame Contents**\n",
        "   - `df.show()`\n",
        "     - This line prints the contents of the `df` DataFrame to the console or notebook output.\n",
        "\n",
        "By using the `versionAsOf` option when reading the Delta table, you can query a specific version of the table's data. In this case, `versionAsOf=0` retrieves the initial version of the table, which corresponds to the state of the data before any updates or modifications were made.\n",
        "\n",
        "This time travel capability provided by Delta Lake is particularly useful for auditing, reproducing results, or reverting to a previous state of the data if needed. It leverages Delta Lake's transaction log and data versioning features to maintain a history of changes and make previous versions of the data accessible."
      ],
      "metadata": {
        "id": "mE8HQtGZYTey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(\"/tmp/delta-table\")\n",
        "print(\"querying version 0 of the dataframe\")\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUj517imYXOn",
        "outputId": "8b4a8f83-620d-4a72-9e8d-934088e432a2"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "querying version 0 of the dataframe\n",
            "+---+\n",
            "| id|\n",
            "+---+\n",
            "|  2|\n",
            "|  3|\n",
            "|  4|\n",
            "|  0|\n",
            "|  1|\n",
            "+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should see the first set of data, from before you overwrote it. Time travel takes advantage of the power of the Delta Lake transaction log to access data that is no longer in the table. Removing the version 0 option (or specifying version 1) would let you see the newer data again. For more information, see Query an older snapshot of a table (time travel).\n"
      ],
      "metadata": {
        "id": "fY6XNYZ2YX6X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Writing a Streaming DataFrame to a Delta Table\n",
        "\n",
        "This code demonstrates how to write a streaming DataFrame to a Delta table using Apache Spark's structured streaming capabilities and the Delta Lake library.\n",
        "\n",
        "### Steps\n",
        "\n",
        "1. **Create a Streaming DataFrame**\n",
        "   - `streamingDf = spark.readStream.format(\"rate\").load()`\n",
        "     - This line creates a streaming DataFrame `streamingDf` by reading data from the \"rate\" source, which is a built-in source that generates a stream of data at a specified rate.\n",
        "\n",
        "2. **Transform Streaming DataFrame**\n",
        "   - `streamingDf.selectExpr(\"value as id\")`\n",
        "     - This line applies a transformation to the `streamingDf` DataFrame by selecting the `value` column and renaming it to `id`.\n",
        "\n",
        "3. **Write Streaming DataFrame to Delta Table**\n",
        "   - `stream = streamingDf.selectExpr(\"value as id\").writeStream.format(\"delta\").option(\"checkpointLocation\", \"/tmp/checkpoint\").start(\"/tmp/delta-table\")`\n",
        "     - This line writes the transformed streaming DataFrame to a Delta table located at `/tmp/delta-table`.\n",
        "     - The `writeStream` method is used to initiate the streaming write process.\n",
        "     - `format(\"delta\")` specifies that the data should be written in the Delta Lake format.\n",
        "     - `option(\"checkpointLocation\", \"/tmp/checkpoint\")` sets the checkpoint location for fault-tolerant processing of the streaming query.\n",
        "     - `start(\"/tmp/delta-table\")` starts the streaming query and writes the data to the specified Delta table path.\n",
        "     - The streaming query is assigned to the `stream` variable for later reference (e.g., to stop the stream).\n",
        "\n",
        "By writing the streaming DataFrame to a Delta table, you can take advantage of Delta Lake's features, such as ACID transactions, data versioning, and efficient storage and retrieval. Additionally, Delta Lake's support for streaming ingestion ensures that the data is written in an efficient and fault-tolerant manner.\n",
        "\n",
        "The `checkpointLocation` option is used to specify a location where Spark can save the progress and metadata of the streaming query, enabling fault-tolerance and allowing the query to recover from failures or restarts."
      ],
      "metadata": {
        "id": "bpZ5q0v0Yn8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "streamingDf = spark.readStream.format(\"rate\").load()\n",
        "stream = streamingDf.selectExpr(\"value as id\").writeStream.format(\"delta\").option(\"checkpointLocation\", \"/tmp/checkpoint\").start(\"/tmp/delta-table\")"
      ],
      "metadata": {
        "id": "UNRJAXP-Y1ml"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "While the stream is running, you can read the table using the earlier commands.\n",
        "\n"
      ],
      "metadata": {
        "id": "3XLJuL_PY7fv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stopping the Streaming Query\n",
        "\n",
        "This line of code stops the streaming query that was previously started to write data to a Delta table.\n",
        "\n",
        "### Stop Streaming Query\n",
        "- `stream.stop()`\n",
        "  - This line invokes the `stop()` method on the `stream` object, which represents the streaming query.\n",
        "  - The `stop()` method gracefully stops the streaming query and releases any resources associated with it.\n",
        "\n",
        "When working with streaming queries in Apache Spark, it is essential to stop the stream when you no longer need it or when you want to terminate the application. Failing to stop the stream can lead to resource leaks and prevent the application from exiting properly.\n",
        "\n",
        "By calling `stream.stop()`, you ensure that the streaming query is terminated safely, and any associated resources, such as threads or network connections, are properly cleaned up. This is a good practice to follow when working with streaming applications to avoid resource leaks and ensure proper application shutdown."
      ],
      "metadata": {
        "id": "dCGMJqGGZBZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stream.stop()"
      ],
      "metadata": {
        "id": "74_xuJpHZLKZ"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reading from Delta Table and Writing to Console\n",
        "\n",
        "This code demonstrates how to read data from a Delta table as a streaming DataFrame and write it to the console using Apache Spark's structured streaming capabilities.\n",
        "\n",
        "### Steps\n",
        "\n",
        "1. **Create a Streaming DataFrame by Reading from Delta Table**\n",
        "   - `spark.readStream.format(\"delta\").load(\"/tmp/delta-table\")`\n",
        "     - This line creates a streaming DataFrame by reading data from the Delta table located at `/tmp/delta-table`.\n",
        "     - The `readStream` method is used to initiate the streaming read process.\n",
        "     - `format(\"delta\")` specifies that the data should be read from a Delta Lake table.\n",
        "     - `load(\"/tmp/delta-table\")` loads the data from the specified Delta table path.\n",
        "\n",
        "2. **Write Streaming DataFrame to Console**\n",
        "   - `.writeStream.format(\"console\").start()`\n",
        "     - This line writes the streaming DataFrame to the console.\n",
        "     - The `writeStream` method is used to initiate the streaming write process.\n",
        "     - `format(\"console\")` specifies that the data should be written to the console output.\n",
        "     - `start()` starts the streaming query and begins writing the data to the console.\n",
        "     - The streaming query is assigned to the `stream2` variable for later reference (e.g., to stop the stream).\n",
        "\n",
        "By reading from a Delta table as a streaming DataFrame, you can continuously monitor changes or updates to the Delta table. This can be useful in scenarios where you want to process data as it arrives or get a real-time view of the data in the Delta table.\n",
        "\n",
        "Writing the streaming DataFrame to the console allows you to inspect the data as it is being processed by the streaming query. This can be helpful for debugging purposes or for getting a quick glimpse of the data in the Delta table.\n",
        "\n",
        "Note that writing to the console is typically used for testing and development purposes. In production scenarios, you would likely write the streaming DataFrame to another data sink, such as another Delta table, a file system, or a database."
      ],
      "metadata": {
        "id": "_vNlgM0RZNBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stream2 = spark.readStream.format(\"delta\").load(\"/tmp/delta-table\").writeStream.format(\"console\").start()\n"
      ],
      "metadata": {
        "id": "06QomjcCZWIV"
      },
      "execution_count": 163,
      "outputs": []
    }
  ]
}