{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNg5gYD5ZQ4oD2tKShG6uKY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samas-it-services/open-course-delta-lake/blob/master/Quickstart.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Delta Lake Quick Start\n",
        "This notebook provides a quick start guide to using Delta Lake, as outlined in the [Delta Lake Documentation](https://docs.delta.io/latest/quick-start.html).\n"
      ],
      "metadata": {
        "id": "rwPBbktNW4JW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVTcjqttVjlt",
        "outputId": "60d7707d-b6f3-43cd-8287-4f86d1bc3be1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark==3.3.1\n",
            "  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.5 (from pyspark==3.3.1)\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845493 sha256=cc533cfab430c4a9d63340b0bb9e79fe65d83fda18fc58a03d1995bba474d6c4\n",
            "  Stored in directory: /root/.cache/pip/wheels/0f/f0/3d/517368b8ce80486e84f89f214e0a022554e4ee64969f46279b\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "  Attempting uninstall: py4j\n",
            "    Found existing installation: py4j 0.10.9.7\n",
            "    Uninstalling py4j-0.10.9.7:\n",
            "      Successfully uninstalled py4j-0.10.9.7\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n",
            "Collecting delta-spark\n",
            "  Downloading delta_spark-3.2.0-py3-none-any.whl (21 kB)\n",
            "Collecting pyspark<3.6.0,>=3.5.0 (from delta-spark)\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: importlib-metadata>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from delta-spark) (8.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=1.0.0->delta-spark) (3.19.2)\n",
            "Collecting py4j==0.10.9.7 (from pyspark<3.6.0,>=3.5.0->delta-spark)\n",
            "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=e79929f8901ac1b47b1d24af2b18ed3b1d222db8304b3a8523053fb513b29c9b\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark, delta-spark\n",
            "  Attempting uninstall: py4j\n",
            "    Found existing installation: py4j 0.10.9.5\n",
            "    Uninstalling py4j-0.10.9.5:\n",
            "      Successfully uninstalled py4j-0.10.9.5\n",
            "  Attempting uninstall: pyspark\n",
            "    Found existing installation: pyspark 3.3.1\n",
            "    Uninstalling pyspark-3.3.1:\n",
            "      Successfully uninstalled pyspark-3.3.1\n",
            "Successfully installed delta-spark-3.2.0 py4j-0.10.9.7 pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "# Install necessary dependencies\n",
        "!pip install pyspark==3.3.1\n",
        "!pip install delta-spark\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from delta import *\n"
      ],
      "metadata": {
        "id": "5dWjuR5NXBQR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Spark Session with Delta Lake\n",
        "builder = SparkSession.builder.appName(\"DeltaLakeQuickStart\") \\\n",
        "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
        "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n"
      ],
      "metadata": {
        "id": "WrEqla6GXDFT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Delta Table\n",
        "data = spark.range(0, 5)\n",
        "data.write.format(\"delta\").save(\"/tmp/delta-table\")\n"
      ],
      "metadata": {
        "id": "Ro0IwOg3XM_T"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read from the Delta Table\n",
        "df = spark.read.format(\"delta\").load(\"/tmp/delta-table\")\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2VpEZw3XOjT",
        "outputId": "5e2d5f18-89d1-4fb0-c437-5858e9b693a8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+\n",
            "| id|\n",
            "+---+\n",
            "|  2|\n",
            "|  3|\n",
            "|  4|\n",
            "|  0|\n",
            "|  1|\n",
            "+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Update the Delta Table\n",
        "data = spark.range(5, 10)\n",
        "data.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/delta-table\")\n"
      ],
      "metadata": {
        "id": "sKAQ9UIBXTEK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the Updated Delta Table\n",
        "df = spark.read.format(\"delta\").load(\"/tmp/delta-table\")\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAq9vT_zXX2Z",
        "outputId": "bb64c98c-753a-4e7c-d686-7c6215c83c22"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+\n",
            "| id|\n",
            "+---+\n",
            "|  7|\n",
            "|  8|\n",
            "|  9|\n",
            "|  5|\n",
            "|  6|\n",
            "+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conditional update without overwrite\n",
        "Delta Lake provides programmatic APIs to conditional update, delete, and merge (upsert) data into tables. Here are a few examples."
      ],
      "metadata": {
        "id": "Ptxun4E4X9gI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from delta.tables import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "deltaTable = DeltaTable.forPath(spark, \"/tmp/delta-table\")\n",
        "\n",
        "# Update every even value by adding 100 to it\n",
        "deltaTable.update(\n",
        "  condition = expr(\"id % 2 == 0\"),\n",
        "  set = { \"id\": expr(\"id + 100\") })\n",
        "\n",
        "# Delete every even value\n",
        "deltaTable.delete(condition = expr(\"id % 2 == 0\"))\n",
        "\n",
        "# Upsert (merge) new data\n",
        "newData = spark.range(0, 20)\n",
        "\n",
        "deltaTable.alias(\"oldData\") \\\n",
        "  .merge(\n",
        "    newData.alias(\"newData\"),\n",
        "    \"oldData.id = newData.id\") \\\n",
        "  .whenMatchedUpdate(set = { \"id\": col(\"newData.id\") }) \\\n",
        "  .whenNotMatchedInsert(values = { \"id\": col(\"newData.id\") }) \\\n",
        "  .execute()\n",
        "\n",
        "deltaTable.toDF().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpRomij7X7UN",
        "outputId": "27f0be7e-5430-419d-9ada-31afb9db229d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+\n",
            "| id|\n",
            "+---+\n",
            "|  0|\n",
            "|  1|\n",
            "|  2|\n",
            "|  3|\n",
            "|  4|\n",
            "|  5|\n",
            "|  6|\n",
            "|  7|\n",
            "|  8|\n",
            "|  9|\n",
            "| 10|\n",
            "| 11|\n",
            "| 12|\n",
            "| 13|\n",
            "| 14|\n",
            "| 15|\n",
            "| 16|\n",
            "| 17|\n",
            "| 18|\n",
            "| 19|\n",
            "+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should see that some of the existing rows have been updated and new rows have been inserted.\n",
        "\n",
        "For more information on these operations, see Table deletes, updates, and merges.\n",
        "\n"
      ],
      "metadata": {
        "id": "s-rCbLVNYGtH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read older versions of data using time travel\n",
        "You can query previous snapshots of your Delta table by using time travel. If you want to access the data that you overwrote, you can query a snapshot of the table before you overwrote the first set of data using the versionAsOf option.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mE8HQtGZYTey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(\"/tmp/delta-table\")\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUj517imYXOn",
        "outputId": "c381cb07-8ffc-4f50-e984-81885bd64ab0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+\n",
            "| id|\n",
            "+---+\n",
            "|  2|\n",
            "|  3|\n",
            "|  4|\n",
            "|  0|\n",
            "|  1|\n",
            "+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should see the first set of data, from before you overwrote it. Time travel takes advantage of the power of the Delta Lake transaction log to access data that is no longer in the table. Removing the version 0 option (or specifying version 1) would let you see the newer data again. For more information, see Query an older snapshot of a table (time travel).\n"
      ],
      "metadata": {
        "id": "fY6XNYZ2YX6X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Write a stream of data to a table\n",
        "You can also write to a Delta table using Structured Streaming. The Delta Lake transaction log guarantees exactly-once processing, even when there are other streams or batch queries running concurrently against the table. By default, streams run in append mode, which adds new records to the table:"
      ],
      "metadata": {
        "id": "bpZ5q0v0Yn8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "streamingDf = spark.readStream.format(\"rate\").load()\n",
        "stream = streamingDf.selectExpr(\"value as id\").writeStream.format(\"delta\").option(\"checkpointLocation\", \"/tmp/checkpoint\").start(\"/tmp/delta-table\")"
      ],
      "metadata": {
        "id": "UNRJAXP-Y1ml"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "While the stream is running, you can read the table using the earlier commands.\n",
        "\n"
      ],
      "metadata": {
        "id": "3XLJuL_PY7fv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# stop the stream\n",
        "You can stop the stream by running stream.stop() in the same terminal that started the stream.\n",
        "\n",
        "For more information about Delta Lake integration with Structured Streaming, see Table streaming reads and writes. See also the Structured Streaming Programming Guide on the Apache Spark website.\n",
        "\n"
      ],
      "metadata": {
        "id": "dCGMJqGGZBZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stream.stop()"
      ],
      "metadata": {
        "id": "74_xuJpHZLKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read a stream of changes from a table\n",
        "While the stream is writing to the Delta table, you can also read from that table as streaming source. For example, you can start another streaming query that prints all the changes made to the Delta table. You can specify which version Structured Streaming should start from by providing the startingVersion or startingTimestamp option to get changes from that point onwards. See Structured Streaming for details.\n"
      ],
      "metadata": {
        "id": "_vNlgM0RZNBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stream2 = spark.readStream.format(\"delta\").load(\"/tmp/delta-table\").writeStream.format(\"console\").start()\n"
      ],
      "metadata": {
        "id": "06QomjcCZWIV"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}